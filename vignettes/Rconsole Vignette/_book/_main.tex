% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Using FishSET in the RConsole},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Using FishSET in the RConsole}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{welcome-to-fishset}{%
\section{Welcome to FishSET}\label{welcome-to-fishset}}

This chapter details the goals of FishSET, outlines the different chapters of this document, and provides instructions for installing FishSET.

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The Spatial Economics Toolbox for Fisheries (FishSET) is a set of statistical programming and data management tools developed to achieve the following goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Standardize data management and organization.
\item
  Provide easily accessible tools to enable location choice models to provide input to the management of key fisheries.
\item
  Organize statistical code so that predictions of fisher behavior developed by the field's leading innovators can be incorporated and transparent to all users.
\end{enumerate}

The FishSET package provides functions for:

\begin{itemize}
\tightlist
\item
  Data management
\item
  Data analysis
\item
  Visualizing data
\item
  Mapping fishing effort
\item
  Statistical modeling of fisher behavior
\item
  Policy comparisons
\item
  Reproducibility
\end{itemize}

FishSET is designed for reproducibility.

\begin{itemize}
\tightlist
\item
  All input and output are stored in the FishSETFolder directory and organized by projects.
\item
  Data is stored in \emph{fishset\_db} SQLite databases, the FishSET database.
\item
  Function calls and notes are stored in a \emph{src} folder.
\item
  Output (plots, table, messages, notes) are saved in an \emph{Output} folder.
\end{itemize}

This document contains a brief description of the FishSET functions and assumes familiarity with R.
If you are unfamiliar with R, we recommended running FishSET functions in the FishSET GUI rather than the RConsole. See the \href{FishSET_GUI.html}{FishSET GUI vignette} for more information.

A more detailed manual is available at the \textbf{\emph{\href{WEBSITE\%20URL}{FishSET GitHub site}}}.

\hypertarget{getting-started}{%
\subsection{Getting started}\label{getting-started}}

If not all ready installed, install \href{https://cloud.r-project.org/}{R} and \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio}.
RStudio is not required but is recommended.

To install the FishSET package, all users need the \texttt{devtools} package and Windows users must install \href{https://cran.r-project.org/bin/windows/Rtools/}{Rtools}.

\begin{description}
\tightlist
\item[Install devtools (required)]
\texttt{install.packages("devtools")}\\
\texttt{library(devtools)}
\end{description}

FishSET is provided as a compressed file that can be installed directly from the FishSET GitHUB site (recommended) or
from a locally saved file location.

\begin{description}
\item[Install FishSET]
To install FishSET from GitHUB\\
\texttt{devtools::install\_github("name/FishSET")}

To install FishSET from local file directory
\texttt{devtools::install\_local("PATH/TO/Directory/Containing/FishSET")}\\
\texttt{library(FishSET)}
\end{description}

Dependencies should be automatically installed. This can take awhile.

For more detailed directions and images, see the \href{https://docs.google.com/document/d/1dzXsVt5iWcAQooDDXRJ3XyMoqnSmpZOqirU_f_PnQUM/edit\#heading=h.1fob9te}{Installing R and RStudio} and \href{https://docs.google.com/document/d/1dzXsVt5iWcAQooDDXRJ3XyMoqnSmpZOqirU_f_PnQUM/edit\#heading=h.h8fxguvqf0mn}{Installing FishSET} section of the FishSET Manual.

\#\#Using FishSET
FishSET functions can be called in the R console or through the FishSET Graphical User Interface (FishSET GUI). This document details how to use FishSET in the R Console. The FishSET GUI is detailed in the \emph{FishSET GUI} vignette. Background and greater detail on functions can be found in the \href{https://docs.google.com/document/d/1dzXsVt5iWcAQooDDXRJ3XyMoqnSmpZOqirU_f_PnQUM/edit?usp=sharing}{FishSET Help Manual}.

\hypertarget{what-is-a-project}{%
\subsection{What is a project?}\label{what-is-a-project}}

Data and output are organized into \emph{projects}. A \texttt{project} is a unique identifier for a set of data and analyses. \texttt{Project} names can be any set of alpha-numeric characters. For instance, a \texttt{project} can be called \emph{pollock} or \emph{pollock2020}. All input and output are saved to a directory with the name of the \texttt{project} within the FishSETFolder directory.

\hypertarget{chapter-overview}{%
\subsection{Chapter overview}\label{chapter-overview}}

\hypertarget{chapter-2-data}{%
\subsubsection{\texorpdfstring{Chapter 2 \protect\hyperlink{data}{Data}}{Chapter 2 Data}}\label{chapter-2-data}}

This chapter goes over the types of data used in FishSET and how to load the data into FishSET. The chapter then outlines functions for conducting basic data quality checks and fixing any issues that arise.

\hypertarget{chapter-3-data-exploration}{%
\subsubsection{\texorpdfstring{Chapter 3 \protect\hyperlink{data-exploration}{Data Exploration}}{Chapter 3 Data Exploration}}\label{chapter-3-data-exploration}}

This chapter goes over the functions to visualize and explore the data.

\hypertarget{chapter-4-spatial-functions}{%
\subsubsection{\texorpdfstring{Chapter 4 \protect\hyperlink{spatial-functions}{Spatial Functions}}{Chapter 4 Spatial Functions}}\label{chapter-4-spatial-functions}}

This chapter outlines the numerous functions related to visualizing the data spatially and generating plots of the spatial distribution of the data.

\hypertarget{chapter-5-creating-and-modifying-data}{%
\subsubsection{\texorpdfstring{Chapter 5 \protect\hyperlink{creating-and-modifying-data}{Creating and Modifying Data}}{Chapter 5 Creating and Modifying Data}}\label{chapter-5-creating-and-modifying-data}}

This chapter outlines the functions to generate new variables, such as catch per unit effort, or modify existing variables, such as extracting year from a date variable.

\hypertarget{chapter-6-modeling-functions}{%
\subsubsection{\texorpdfstring{Chapter 6 \protect\hyperlink{modeling-functions-1}{Modeling Functions}}{Chapter 6 Modeling Functions}}\label{chapter-6-modeling-functions}}

This chapter goes over the steps required to generate the data used in the models, define the models, run the models, and compare output.

\hypertarget{chapter-7-policy-and-welfare-analysis}{%
\subsubsection{\texorpdfstring{Chapter 7 \protect\hyperlink{policy-and-welfare-analysis}{Policy and Welfare Analysis}}{Chapter 7 Policy and Welfare Analysis}}\label{chapter-7-policy-and-welfare-analysis}}

The chapter goes over how to define policy scenarios and run welfare analyses.

\hypertarget{chapter-8-reporting}{%
\subsubsection{\texorpdfstring{Chapter 8 \protect\hyperlink{reporting}{Reporting}}{Chapter 8 Reporting}}\label{chapter-8-reporting}}

This chapter goes over how to open and use the report template.

\hypertarget{data}{%
\section{Data}\label{data}}

\hypertarget{required-and-optional-data}{%
\subsection{Required and optional data}\label{required-and-optional-data}}

FishSET uses five data types.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
File Type\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
Primary data\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Required. Contains the main data for the model.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
Port data\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Required. Contains latitude and longitude locations of ports.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
Spatial data\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Required. File defining boundaries of fishery or regulatory zones.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
Gridded data\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Optional. Contains additional data that varies by two dimensions.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Example, sea surface temperature measure\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
Auxiliary data\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Optional. Contains additional data to link to the primary data.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.21}}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 1\tabcolsep) * \real{0.79}}\raggedright
Example, vessel characteristics.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

All optional data files must be linkable to the primary data file.

Gridded data should link to the fishery or regulatory areas in the spatial data. Auxilary data must link to a variable in the primary data table.

FishSET will accept several data formats including:

\begin{itemize}
\tightlist
\item
  csv files
\item
  geojson files
\item
  json files
\item
  matlab files
\item
  R files\\
\item
  shape files
\item
  spss files
\item
  stata files
\item
  txt files
\item
  xls/xlsx files
\end{itemize}

Details on the different data types are detailed in the \href{https://docs.google.com/document/d/1dzXsVt5iWcAQooDDXRJ3XyMoqnSmpZOqirU_f_PnQUM/edit\#heading=h.36sb14g0441y}{Data Types} section of the FishSET manual.

\hypertarget{loading-and-reading-data}{%
\subsection{Loading and reading data}\label{loading-and-reading-data}}

\hypertarget{loading-data-to-the-fishset-database}{%
\subsubsection{Loading data to the FishSET database}\label{loading-data-to-the-fishset-database}}

All data should be saved to the FishSET database. Doing so ensures that data for a project are stored together.

The \emph{load} functions check that the data file can be saved to the FishSET database and, if all checks pass, saves the data file to the FishSET database.

When the primary data is saved, two versions of the data are created, a raw table with the original data and a working table on which all analyses will be conducted. The working table will also be available in the global environment. Revised data can be saved as interim tables in the database along with a ``finalized'' data table (data table used for models).

\hypertarget{functions}{%
\paragraph{Functions}\label{functions}}

\begin{verbatim}
 load_maindata() Primary data file. 
 load_port()     Port data file. 
 load_aux ()     Auxiliary data file. 
 load_grid()     Gridded data file.
\end{verbatim}

\hypertarget{function-details}{%
\paragraph{Function details}\label{function-details}}

\texttt{load\_maindata(dat,\ project,\ over\_write\ =\ TRUE,\ compare\ =\ FALSE,\ y\ =\ NULL)} read in a data file, then parses it, and finallys saves the data table to the FishSET database if no errors are found. The function calls the \texttt{data\_verification} function to check for common data quality issues and that latitude and longitude are defined in \texttt{dat}. If no issues are found, the data is saved in the FishSET database for the \texttt{project} as \texttt{projectMainDataTable} and loaded into the working environment. If \texttt{over\_write\ =TRUE}, the existing data table for the project, if exists in the FishSET database, will be overwritten. If \texttt{compare\ =\ TRUE} the \texttt{fishset\_compare} function is called and \texttt{dat} is compared with \texttt{y}, a previously saved version of \texttt{dat}.

\texttt{load\_port}

\texttt{load\_spatial}

\texttt{load\_aux}

\texttt{load\_grid}

\begin{quote}
``Add details about loading port, spatial , auxiliary, and gridded files.''
\end{quote}

\hypertarget{reading-data-into-r}{%
\subsubsection{Reading data into R}\label{reading-data-into-r}}

Data can also be read into R without saving it to the FishSET database. This function should only be used to view and evaluate data tables. If the data is going to be used in FishSET it should be loaded into the FishSET database.

To read data files into the working environment use:

\begin{verbatim}
 read_dat(x, data.type = NULL, is.map = F, ...). 
\end{verbatim}

\hypertarget{function-details-1}{%
\paragraph{Function details}\label{function-details-1}}

\texttt{read\_dat(x,\ data.type=NULL,\ is.map\ =\ FALSE,\ drv\ =\ NULL,\ dbname\ =\ NULL,\ user\ =\ NULL,\ password\ =\ NULL,\ ...)} import data from local file directory or webpage into the R environment. \texttt{x}is the name and path, or web address, of the dataset to be read in. \texttt{data.type} is the file extension. It is optional as the function will attempt to detect the file extension. \texttt{data.type} must be defined if \texttt{x} is the path to a shape folder, if \texttt{x} is a Google spreadsheet (\texttt{data.type\ =\ \textquotesingle{}google\textquotesingle{})}, or if the correct extension cannot be derived from \texttt{x}. Set \texttt{is.map\ =\ TRUE} only for spatial files in a .json file extension. For sql files, additional arguments may be required, including the database driver (\texttt{drv}), the database name (\texttt{dbname}), and user name (\texttt{user}) and password (\texttt{password}) for the SQL database.

\texttt{read\_dat()} is flexible and will accommodate several data extensions and additional format-specific arguments. It is a wrapper function that calls a file extension-specific function to read in the data. \texttt{x} is the name and path of the dataset to be read in. \texttt{data.type} is optional argument to specify the file extension. If \texttt{data.type} is not specified, the detected data extension in \texttt{x} will be used. \texttt{is.map} is a logical argument specifying if \texttt{x} is a spatial data file. is.map must be set to TRUE if a spatial data file is not in a geojson or shape file. \texttt{...} is used to add additional extension-specific arguments. See function documentation for more details.

\hypertarget{loading-data-from-fishset-database-into-the-the-working-environment}{%
\subsubsection{Loading data from FishSET database into the the working environment}\label{loading-data-from-fishset-database-into-the-the-working-environment}}

Data saved to the FishSET database can be reloaded into the working environment using

\begin{verbatim}
 load_data(project, name=NULL). 
\end{verbatim}

Specify the project name (\texttt{project}) and, optionally, the table name (\texttt{name}). \texttt{name} is optional and is used to specify a specific version of the data table. For example,

\begin{verbatim}
 load_data("pollock", name="pollockMainDataTable20200101")   
\end{verbatim}

loads the primary data from project \emph{pollock} that was saved on January 1st, 2020. If \texttt{name} is NULL, the undated working table is pulled.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#Load in the data from the pollock project}
\FunctionTok{library}\NormalTok{(FishSET)}
\FunctionTok{load\_data}\NormalTok{(}\StringTok{"pollock"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{merging-gridded-and-auxiliary-data-with-the-primary-data}{%
\subsection{Merging gridded and auxiliary data with the primary data}\label{merging-gridded-and-auxiliary-data-with-the-primary-data}}

Secondary data (auxiliary, port, gridded) can be merged into the primary dataset. The default method to merge is to use a left join; all columns and rows from the primary dataset are kept whereas only matching columns and rows from the secondary table are joined. Although all secondary data types can be merged into the primary day, generally only auxiliary data (such as vessel characteristics) will need to be merged.

\hypertarget{functions-1}{%
\paragraph{Functions}\label{functions-1}}

\begin{verbatim}
merge_dat()   Merge secondary data into the primary data tahble
split_dat()   Split and save secondary data from the primary data table
\end{verbatim}

\hypertarget{function-details-2}{%
\paragraph{Function details}\label{function-details-2}}

\texttt{merge\_dat(dat,\ other,\ project,\ main\_key,\ other\_key,\ other\_type\ =\ NULL,\ merge\_type\ =\ "left")} merges secondary data (\texttt{other}) to the primary data (\texttt{dat}) based on one or more variables for \texttt{dat} (\texttt{main\_key}) and \texttt{other} (\texttt{other\_key}). If more than one variable is being used for merging, include as a list. Variable names do not have to be the same between \texttt{main\_key} and \texttt{other\_key} but they must be included in the same order. \texttt{other\_type} is optional and defines the data type of the secondary data (``aux'', ``grid'', ``spat'', ``port''). \texttt{merge\_type} can be ``left'' or ``full''. ``left'' keeps all columns and rows in \texttt{dat} but only the rows from \texttt{other} in which the column values match column values in \texttt{dat}. ``full'' keeps all rows of both \texttt{dat} and \texttt{other.} ``left'' merge is preferred as using ``full'' can result in a larger frequency of missing data if the secondary data contains observations beyond the scope of \texttt{dat}. For example, \texttt{dat} contains catch data only on catcher vessels but other contains vessel characteristics for catcher vessels, mothership vessels, and catch processor vessels.

\texttt{split\_dat(dat,\ project,\ aux\ =\ NULL,\ split\_by\ =\ NULL,\ key,\ output\ =\ "main")} separates secondary data (auxiliary, gridded, or port data) from the MainDatatable. Identify data to remove from \texttt{dat} using an existing secondary data table or list of column names. If \texttt{aux} is specified, the column names in \texttt{dat} that match the column names in \texttt{aux} are separated from \texttt{dat}. If \texttt{aux\ =\ NULL} and \texttt{split\_by} is a list, then column names in \texttt{dat} matching \texttt{split\_by} are separated from \texttt{dat}. \texttt{key} is the columns that link \texttt{dat} and \texttt{aux.} If \texttt{aux} is null then \texttt{key} should be a column name from \texttt{split\_by}. The function can return just the primary data (\texttt{output\ =\ “main”}), just the separated table (\texttt{output\ =\ “aux”}), or both, in a list (\texttt{output\ =\ “both”}).

\hypertarget{working-in-the-fishset-database}{%
\subsection{Working in the FishSET database}\label{working-in-the-fishset-database}}

The functions in this section interface with the FishSET database to display the requested output in the R Console. The functions are wrappers for functions in the \textbf{DBI} package. They allow users to focus on the data and not on managing the database or learning SQL.

Basic functions:

\begin{verbatim}
 tables_database(project)        View list of all tables in the FishSET database for the project.
 list_MainDataTables(project)      View list of MainDataTables in FishSET database for the project.
 list_PortTables(project)          View list of PortTables in FishSET database for the project.
 list_logs(project)                View list of all log files for the project.
 table_fields(table, project)    View fields in specified table for the project.
 table_view(table, project)      Display specified table for the project.
 table_exists(table, project)      Check if table exists in the FishSET database for the project.
 table_remove(table, project)      Remove table from the FishSET database for the project.
\end{verbatim}

Model output files can be viewed in the FishSET database using:

\begin{verbatim}
 globalcheck_view(table, project)  View error output discrete choice models for the project
 model_out_view(table, project)    Load discrete choice model output for the project
\end{verbatim}

\hypertarget{metadata}{%
\subsection{Metadata}\label{metadata}}

\begin{quote}
``This needs to be detailed''
\end{quote}

\hypertarget{data-management-functions}{%
\subsection{Data management functions}\label{data-management-functions}}

Data should, at a minimum, be checked for entry errors and errors resulting from data import and conversion. Users should also investigate and summarize the data to understand potential data and statistical problems.

FishSET provides a number of statistical and graphic functions for evaluating data quality. These functions focus on issues that are most likely to affect model convergence and model performance. They do not cover all potential data quality issues that exist in your data.

\hypertarget{functions-2}{%
\subsubsection{Functions}\label{functions-2}}

\hypertarget{visualizing-the-data}{%
\paragraph{Visualizing the data}\label{visualizing-the-data}}

\begin{verbatim}
 summary_stats()    Table with basic summary statistics.
 spatial_hist()     Histogram of lat and lon by grouping variable.
 spatial_summary()  Line plots of selected variable against date and zone.
 map_plot()         Static map of haul locations.
\end{verbatim}

\hypertarget{subsetting-the-data}{%
\paragraph{Subsetting the data}\label{subsetting-the-data}}

\begin{verbatim}
 select_vars()      Select variables to add back into the working dataset. 
 add_vars()         Add variables from `select_vars` into primary dataset.
\end{verbatim}

\hypertarget{checking-for-outliers}{%
\paragraph{Checking for outliers}\label{checking-for-outliers}}

\texttt{outlier\_boxplot(dat,\ project,\ x\ =\ NULL)} returns a box-and-whisker plot of all numeric variables in \texttt{dat}. Use \texttt{x} to define a set of variables to assess. This function helps to identify which variables potentially contain outlier variables and should be run before outlier\_table() and outlier\_plot() functions, which can be applied to only one variable at a time.

\begin{verbatim}
 outlier_table()    Returns table with quantiles for all numeric variables in the data table.
 outlier_plot()     Returns a plot of the selected variable.
 outlier_remove()   Returns modified data set where outliers have been removed.
\end{verbatim}

\hypertarget{checking-for-empty-variables-unique-rows-nas-and-nans}{%
\paragraph{Checking for empty variables, unique rows, NAs, and NaNs}\label{checking-for-empty-variables-unique-rows-nas-and-nans}}

\begin{verbatim}
 empty_vars         checks for emtpy variables.
 nan_identify()     Checks whether NAs or NaNs are present in any variable.
 nan_filter()       Returns modified data set where NaNs have been removed or replaced.
 na_filter()        Returns modified data set where NAs have been removed or replaced.
 unique_filter()    Check for and remove duplicate rows.
\end{verbatim}

\hypertarget{filtering-rows-of-the-dataset}{%
\paragraph{Filtering rows of the dataset}\label{filtering-rows-of-the-dataset}}

\begin{verbatim}
 filter_table()     Stores filter functions in a table.     
 filter_dat()       Apply stored and new filter functions to data.
\end{verbatim}

\hypertarget{converting-class-or-lonlat-units}{%
\paragraph{Converting class or lon/lat units}\label{converting-class-or-lonlat-units}}

\begin{verbatim}
 changeclass()       Convert variable class.
 degree()            Convert lon/lat to decimal format.
\end{verbatim}

\hypertarget{wrappers-for-multiple-checking-functions}{%
\paragraph{Wrappers for multiple ``checking'' functions}\label{wrappers-for-multiple-checking-functions}}

\begin{verbatim}
 data_check()        Wrapper for data quality check functions, including data_verification().
 data_verification() Checks for unique column names, each row is a unique choice occurrence, etc.
 check_model_data()  Checks for presence of NAs, NaNs and Inf in model data. Function must be run before models can be developed.
\end{verbatim}

\hypertarget{function-details-3}{%
\subsubsection{Function details}\label{function-details-3}}

\hypertarget{visualizing-the-data-1}{%
\paragraph{Visualizing the data}\label{visualizing-the-data-1}}

\texttt{summary\_stats(dat,\ project,\ x\ =\ NULL)} prints summary statistics for each variable in the \texttt{dat}. If \texttt{x} is specified, summary stats will be returned only for that variable.

\begin{verbatim}
 spatial_hist()    
 spatial_summary()  
 map_plot()         
\end{verbatim}

\hypertarget{subsetting-the-data-1}{%
\paragraph{Subsetting the data}\label{subsetting-the-data-1}}

\begin{verbatim}
 select_vars()       
 add_vars()         
\end{verbatim}

\hypertarget{checking-for-outliers-1}{%
\paragraph{Checking for outliers}\label{checking-for-outliers-1}}

\begin{verbatim}
 outlier_boxplot
 outlier_table()    
 outlier_plot()     
 outlier_remove()   
\end{verbatim}

\hypertarget{checking-for-empty-variables-unique-rows-nas-and-nans-1}{%
\paragraph{Checking for empty variables, unique rows, NAs, and NaNs}\label{checking-for-empty-variables-unique-rows-nas-and-nans-1}}

\texttt{empty\_vars\_filter(dat,\ project,\ remove\ =\ FALSE)} checks for empty variables and prints an outcome message to the console. If empty variables are present and \texttt{remove\ =\ TRUE}, then empty variables will be removed from \texttt{dat}.

\begin{verbatim}
 nan_identify()     
 nan_filter()  
 
\end{verbatim}

\texttt{na\_filter(dat,\ project,\ x=NULL,\ replace\ =\ F,\ remove\ =\ F,\ rep.value\ =\ NA,\ over\_write\ =\ FALSE)} checks for NAs and removes or replaces NAs. To check for NAs across \texttt{dat}, run the function specifying only \texttt{dat} {[}\texttt{na\_filter(pollockMainDataTable)}{]}. The function will return a statement of which variables, if any, contain NAs. To remove NAs, use \texttt{remove\ =\ TRUE}. All rows containing NAs in variable(s) \texttt{x} will be removed from \texttt{dat}. To replace NAs, use \texttt{replace\ =\ TRUE}. If \texttt{replace\ =\ TRUE} and \texttt{rep.value} is not defined, then NAs are replaced with the mean value of \texttt{x}. \texttt{rep.value} must be specified if \texttt{x} is not numeric. The function returns the modified dataset if either \texttt{replace} or \texttt{remove} are true. Save the modified data table to the FishSET database by setting \texttt{over\_write\ =\ TRUE}.

\texttt{unique\_filter(dat,\ project,\ remove\ =\ FALSE)} returns a statement of whether each row in \texttt{dat} is unique. Set \texttt{remove=TRUE} to remove duplicate rows.

\hypertarget{filtering-rows-of-the-dataset-1}{%
\paragraph{Filtering rows of the dataset}\label{filtering-rows-of-the-dataset-1}}

\begin{verbatim}
 filter_table()      
 filter_dat()     
\end{verbatim}

\hypertarget{converting-variable-class-or-lonlat-units}{%
\paragraph{Converting variable class or lon/lat units}\label{converting-variable-class-or-lonlat-units}}

\texttt{changeclass(dat,\ project,\ x\ =\ NULL,\ newclass\ =\ NULL,\ savedat\ =\ FALSE)} returns a table with data class for each variable in \texttt{dat} and changes variable classes. To view variable classes run the function with default settings, specifying only \texttt{dat} and \texttt{project}. If variable class should be changed, run the function again, specifying the variables (\texttt{x}) to be change and the newclasses (\texttt{newclass}). Length of \texttt{newclass} should match the length of \texttt{x} unless all variables in \texttt{x} should be the same class. Options are ``numeric'', ``factor'', ``date'', and ``character''. Use \texttt{savedat\ =\ TRUE} to save modified data table.

\begin{verbatim}
 degree()            
\end{verbatim}

\hypertarget{wrappers-for-multiple-checking-functions-1}{%
\paragraph{Wrappers for multiple ``checking'' functions}\label{wrappers-for-multiple-checking-functions-1}}

\texttt{data\_check()} is the primary function to check for data quality issues in the dataset. The function calls \texttt{summary\_stats}, \texttt{nan\_identify}, \texttt{outlier\_table} and \texttt{outlier\_plot}, and \texttt{data\_verification}, which contains script to check for unique column names and empty columns, that each row is a unique observation at haul or trip level, and calls the \texttt{degree} function. Use \texttt{outlier\_remove}, \texttt{na\_filter}, \texttt{nan\_filter}, and \texttt{degree} to correct any errors. Empty columns and duplicate rows will be removed.

\texttt{data\_verification(dat,\ project)} checks that all column names in the \texttt{dat} are unique, whether any columns in the data frame are empty, whether each row is a unique choice occurrence at the haul or trip level, and that either latitude and longitude or fishing area are included.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#data\_check(pollockMainDataTable, \textquotesingle{}pollock\textquotesingle{}, \textquotesingle{}OFFICIAL\_TOTAL\_CATH\_MT\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\texttt{check\_model\_data} checks for data quality issues that will result in errors when running models, including NAs, NaNs and INF values in the dataset and that each row is a unique occurrence at the haul or trip level. Run this function before creating the model design file (\texttt{make\_model\_design}). Even if the data passed the \texttt{data\_check} tests, errors may be produced by data modification and data creation functions.

Use \texttt{filter\_table} and \texttt{filter\_dat} to subset the data, retaining rows that meet the condition. Define and save, for records and future use, conditions in \texttt{filter\_table}. Apply filters with \texttt{filter\_dat}.

\begin{longtable}[]{@{}lll@{}}
\toprule
Data table & Vector & FilterFunction\tabularnewline
\midrule
\endhead
MainDataTable & ``PORT\_CODE'' & PORT\_CODE == 1\tabularnewline
MainDataTable & ``TRIP\_START'' & TRIP\_START \textgreater= 2011-02-01\tabularnewline
PortDataTable & ``LATITUDE'' & LATITUDE \textless{} 57\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{data-exploration}{%
\section{Data Exploration}\label{data-exploration}}

Several functions are available for summarizing and visualizing data.

We group exploratory data functions into 1) basic exploratory analysis - functions exploring the distribution, availability, and variance of data, 2) fleet summaries -- functions to view and understand fleets and identify meaningful groupings, and 3) simple analyses - functions to evaluate relationships between variables and identify redundant variables.

All output is saved to the output folder.

\hypertarget{data-exploration-functions}{%
\subsection{Data Exploration functions}\label{data-exploration-functions}}

Basic exploratory analysis

\begin{verbatim}
map_kernel()      Spatial kernel density plot.  
getis_ord_stats() Getis-Ord statistic.  
morans_stats()    Moran’s I statistic.  
map_plot()        Static map of haul locations. 
map_viewer()      Interactive map of haul locations or paths with zone. 
spatial_summary() View aggregated variable by date and fishing zone.    
spatial_hist()    Assess spatial variance/clumping of grouping variable.    
temp_plot()       View distribution of variable over time.  
\end{verbatim}

Fleet summaries

\begin{verbatim}
vessel_count()    Counts of unique vessels active within a specified period.
species_catch()   Aggregates total catch for one or more species. 
weekly_catch()    Calculates weekly catch for one or more species.
weekly_effort()   Generates the average CPUE by week. 
density_plot()    Plots the chosen density plot of chosen variable.
bycatch()         Compares mean CPUE and share (or count) of total catch by time period.
roll_catch()      Rolling catch (specify window size and summary statistic).    
trip_length()     View trip duration.   
\end{verbatim}

Simple analyses

\begin{verbatim}
corr_out()        View the correlation coefficient between numeric variables.   
xy_plot()         Asses fitted relationship between two variables.  
\end{verbatim}

All tables and plots generated are saved to the output folder.

\#\#\#Function details

\texttt{map\_kernel(dat,\ project,\ type,\ latlon,\ group,\ facet,\ date,\ filter\_date,\ filter\_value,\ minmax)} returns the kernel density plot in the specified plot \texttt{type} (\emph{``point''}, \emph{``contours''}, \emph{``gradient''}) based on the specified latitude and longitude variables (\texttt{latlon}). All other arguments are optional. \texttt{group} is the variable in \texttt{dat} containing group identifiers. If groups are specified, users can map each group as a separate facet if \texttt{facet\ =\ TRUE}. Data can be filtered by a \texttt{date} variable. Select how to filter using \texttt{filter\_date} and values with \texttt{filter\_value}. \texttt{filter\_date} options are \emph{``year''}, \emph{``month''}, and \emph{``year-month''}. \texttt{filter\_value} should be four digits if year and 2 digits if month. Use colon for a range. Use a list if using \emph{``year-month''}, with the format: \emph{list(year(s), month(s))}. For example, \emph{list(2011:2013, 5:7)}. Use \texttt{minmax} to limit the map extent. \texttt{minmax} is a vector of length four corresponding to c(minlat, maxlat, minlon, maxlon).\\
\texttt{getis\_ord\_stats(dat,\ project,\ varofint,\ spat,\ lon.dat,\ lat.dat,\ cat,\ lon.grid,\ lat.grid)} is a wrapper function to calculate global and local Getis-Ord (the degree, within each zone, that high or low values of the \texttt{varofint} cluster in space) by discrete area. Function utilizes the \texttt{localG} and \texttt{knearneigh} functions from the \textbf{spdep} package. The spatial input is a row-standardized spatial weights matrix for computed nearest neighbor matrix, which is the null setting for the \texttt{nb2listw} function. Requires a data frame with area as a factor, the lon/lat centroid for each area, the lon/lat outlining each area, and the variable of interest \texttt{varofint} or a spatial data file with lon/lat defining boundaries of area/zones and variable of interest for weighting. If the centroid is not included in the spatial data file, then \texttt{find\_centroid()} can be called to calculate the centroid of each zone. If the variable of interest is not associated with an area/zone then the \texttt{assignment\_column()} function can be used to assign each observation to a zone. Arguments to identify centroid and assign variable of interest to area/zone are optional and default to NULL.\\
\texttt{morans\_stats(dat,\ project,\ varofint,\ spat,\ lon.dat,\ lat.dat\ =\ NULL,\ cat,\ lon.grid,\ lat.grid)} is a wrapper function to calculate global and local Moran's I (degree of spatial autocorrelation) of a variable (\texttt{varofint}) by discrete area. Function utilizes the \texttt{localmoran()} and \texttt{knearneigh()} functions from the \textbf{spdep} package. The spatial input is a row-standardized spatial weights matrix for computed nearest neighbor matrix, which is the null setting for the \texttt{nb2listw()} function. The function requires a spatial data file with latitude and longitude defining boundaries of area/zones. If zonal centroid is not included in the spatial data file, then FishSET's \texttt{find\_centroid()} function is called. If each observation in \texttt{dat} is not associated with an area/zone, then FishSET's \texttt{assignment\_column()} is called to assign each observation to a zone. Arguments to identify zonal centroids and assign variable of interest to area/zone are optional and default to NULL.\\
\texttt{map\_plot(dat,\ project,\ lat,\ lon,\ minmax,\ percshown)} plots observed vessel locations on a map. \texttt{lon} and \texttt{lat} are the names of variables containing longitude and latitude data in dat. If the predefined map extent needs adjusting, set limits with \texttt{minmax\ =\ c(minlat,\ maxlat,\ minlon,\ maxlon)}. To show a random subset of points, set \texttt{percshown} to the percent of points to show. Please consider any confidentiality concerns before sharing or publishing the map. This graphic return a static but shareable map. See \texttt{map\_viewer()} to view a dynamic map with greater functionality.\\
\texttt{map\_viewer(dat,\ gridfile,\ avd,\ avm,\ num\_vars,\ temp\_vars,\ id\_vars,\ lon\_start,\ lat\_start,\ lon\_end,\ lat\_end)} opens an interactive map in your default web browser to view vessel points or maps. \texttt{gridfile} is required to overlay fishery management/regulatory zones on the map. \texttt{dat} and \texttt{gridfile} are linked through \texttt{avd} (variable in \texttt{dat} containing zone name or identifier) and \texttt{avm} (property in \texttt{gridfile} containing zone name or identifier). Vessel haul points or vessel haul paths can be plotted. \texttt{lon\_start} and \texttt{lat\_start} are the variable names in \texttt{dat} containing the longitude and latitude of the vessel location points to plot or starting location if plotting paths. \texttt{lon\_end} and \texttt{lat\_end} are the ending latitude and longitudes if plotting a path. Leave as NULL if plotting location points. Other plotting and grouping arguments include \texttt{num\_vars} (a list of one or more numeric variables in \texttt{dat}), \texttt{temp\_vars} (a list of one or more temporal variables in \texttt{dat}), and \texttt{id\_var}s (a list of categorical variables in \texttt{dat}). These variables are used to create additional scatter plots for assessing the data. Multiple numeric, temporal, and ID variables can be included but only one will be shown at a time. To close the server connection run \texttt{servr::daemon\_stop()} in the console. Note that it can take up to a minute for the data to be loaded onto the map. The map cannot be saved.\\
\texttt{spatial\_summary(dat,\ project,\ stat.var,\ variable,\ gridfile,\ lon.grid,\ lat.grid,\ lon.dat,\ lat.dat,\ cat)} returns the variable aggregated by \texttt{stat.var} plotted against date and zone. \texttt{gridfile}, \texttt{lon.grid}, \texttt{lat.grid}, \texttt{lon.dat}, \texttt{lat.dat}, and \texttt{cat} are not required if zone assignment variable exists in \texttt{dat}.

\texttt{stat.var} options

\begin{verbatim}
length         Number of observations
no_unique_obs  Number of unique observations
perc_total     Percent of total observations
mean           Mean  
median         Median  
min            Minimum  
max            Maximum  
sum            Sum
\end{verbatim}

\texttt{spatial\_hist(dat,\ project,\ group)} returns a histogram of observed lon/lat split by grouping variable (\texttt{group}). The function is used to assess spatial variance/clumping of selected grouping variable.

\texttt{temp\_plot(dat,\ project,\ var.select,\ len.fun,\ agg.fun,\ date.var)} returns two plots of selected variable \texttt{var.select} over time (\texttt{date.var}). The first plot can show the number of observations (\texttt{len.fun\ =\ "length"}), number of unique observations (\texttt{len.fun\ =\ "unique"}), percent of options (\texttt{len.fun\ =\ "percent"}) of \texttt{var.select} by time. The second plot shows \texttt{var.select} aggregated by \texttt{agg.fun} against date. \texttt{agg.fun} options are \emph{mean}, \emph{median}, \emph{min}, \emph{max}, and \emph{sum}.

\textbf{Fleet and group summaries}

Each function has a different objective but they all have a set of optional arguments that allow for grouping the data (\texttt{group}) or filtering by date (\texttt{filter\_date}) or another value (\texttt{filter\_value}). These options default to NULL and do not have to be specified. There are additional options to adjust the appearance of the output. These options are detailed first; then the functions are detailed.

\begin{description}
\item[\emph{Additional options}]
\texttt{group} is the name of the fleet ID variable or other grouping variable(s). Multiple group variables can be included but only the first two will be shown. For most variables, grouping variables can be merged into one variable using \texttt{combine}; in this case any number of variables can be joined, but no more than three is recommended. For functions that do not have the \texttt{combine} argument, group variables will be automatically combined if three or more are included.

\texttt{date} is the variable name from \texttt{dat} used to subset and/or facet the plot by.

\texttt{filter\_date} is the type of filter to apply to the table. Options include \emph{``date\_range''}, \emph{``year-day''}, \emph{``year-week''}, \emph{``year-month''}, \emph{``year''}, \emph{``month''}, \emph{``week''}, or \emph{``day''}. The \emph{``date\_range''} option will subset the data by two date values entered in \texttt{date\_value}. The argument \texttt{date\_value} must be provided.

\texttt{date\_value} is the value to filter \texttt{date} by. If using \texttt{filter\_date\ =\ "date\_range"}, \texttt{date\_value} should contain a start and end date, e.g.~c(``2011-01-01'', ``2011-03-15''). Otherwise, use integers (4 digits if \emph{year}, 1-2 digits if \emph{day}, \emph{month}, or \emph{week}). Use a list if using a two-part filter, e.g.~``year-week'', with the format \texttt{list(year,\ week)} or a vector if using a single period, \texttt{c(week)}. For example, \texttt{list(2011:2013,\ 5:7)} will filter the data table from weeks 5 through 7 for years 2011-2013 if \texttt{filter\_date\ =\ "year-week"}. \texttt{c(2:5)} will filter the data February through May when \texttt{filter\_date\ =\ "month"}.

\texttt{filter\_by} is the variable name to filter \texttt{dat} by.

\texttt{filter\_value} is a vector of values in the \texttt{filter\_by} variable to filter \texttt{dat}.

\texttt{filter\_expr} a valid R expression to filter \texttt{dat} by using the \texttt{filter\_by} variable.

\texttt{facet\_by} is the variable name to facet by. This can be a variable that exists in the dataset, or a variable created by the function such as \emph{``year''}, \emph{``month''}, or \emph{``week''}. For faceting, any variable (including ones listed in \texttt{group}) can be used, but \emph{``year''} and \emph{``month''} are also available. Currently, combined variables cannot be faceted.

\texttt{tran} is the name of function to transform variable by. Options include \emph{``identity''}, \emph{``log''}, \emph{``log2''}, \emph{``log10''}, and \emph{``sqrt''}. \emph{``identify''} (no transformation) is the default.

\texttt{scale} is passed to facet\_grid defining whether x- and y-axes should be consistent across plots. Defaults to \emph{``fixed''}. Other options include \emph{``free\_y''}, \emph{``free\_x''}, and \emph{``free''}.

\texttt{position} The position of the grouped variable for the plot. Options include \emph{``identity''}, \emph{``stack''}, and \emph{``fill''}.

\texttt{output} display as \emph{``plot''}, \emph{``table''}, or both (\emph{``tab\_plot''}). Defaults to both.

\texttt{format\_tab} defines how table output should be formatted. Options include \emph{``wide''} (the default) and \emph{``long''}.
\end{description}

\emph{Function details}

\texttt{vessel\_count(dat,\ project,\ \ v\_id,\ date,\ period,\ group,\ filter\_date,\ \ date\_value,\ filter\_by,\ filter\_value,\ \ filter\_expr,\ facet\_by,\ combine,\ position,\ \ tran,\ \ value,\ type,\ \ scale,\ output)} aggregates the number (or percent) of active vessels by time period (\texttt{period}) using a column of unique vessel IDs (\texttt{v\_id}). \texttt{period} options include \emph{``year''}, \emph{``month''}, \emph{``weeks''} (weeks in the year), \emph{``weekday''}, \emph{``weekday\_abv''}, \emph{``day''} (day of the month), and \emph{``day\_of\_year''}.\\
\texttt{species\_catch(dat,\ project,\ species,\ date,\ period,\ fun,\ group,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ type,\ conv,\ tran,\ value,\ position,\ combine,\ scale,\ output,\ format\_tab)} aggregates catch by time period (\texttt{period}) and species using one or more columns of catch data (\texttt{species}). \texttt{fun} is the name of the function to aggregate catch by. Defaults to \emph{``sum''}. Use \texttt{conv} to convert catch that is in pounds to \emph{``tons''} or \emph{``metric\_tons''}. Set \texttt{conv\ =\ "none"} if no conversion is desired. Alternative, \texttt{conv} can be a user-defined function. This is useful if catch is not in pounds.\\
\texttt{roll\_catch(dat,\ project,\ catch,\ date,\ group,\ k\ =\ 10,\ fun,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ scale,\ align,\ conv,\ tran,\ output,\ ...)} returns a rolling window time series aggregation of catch that can be combined with grouping variable(s) (\texttt{group}). \texttt{catch} can be one or more columns of catch data. \texttt{k} defines the window size and \texttt{fun} is the name of the function to aggregate catch by. Defaults to \emph{``mean''}. \texttt{align} indicates whether results of window should be left-aligned \emph{(``left'')}, right-aligned \emph{(``right'')}, or centered \emph{(``center'')}. Defaults to \emph{``center''}. Use \texttt{conv} to convert catch that is in pounds to ``tons'' or ``metric\_tons''. Alternative, conv can be a user-defined function. This is useful if catch is not in pounds.\\
\texttt{weekly\_catch(dat,\ project,\ species,\ date,\ fun,\ group,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ type,\ conv,\ tran,\ value,\ position,\ combine,\ scale,\ output,\ format\_tab)} aggregates catch by week using one or more columns of catch data (\texttt{species}). \texttt{fun} is the name of the function to aggregate catch by. Defaults to \emph{``sum''}. Use \texttt{conv} to convert catch that is in pounds to \emph{``tons''} or \emph{``metric\_tons''}. Alternatively, \texttt{conv} can be a user-defined function. This is useful if catch is not in pounds.\\
\texttt{weekly\_effort(dat,\ project,\ cpue,\ date,\ group,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ tran,\ combine,\ scale,\ output,\ format\_tab)} calculates mean CPUE by week. This function doesn't calculate CPUE; the CPUE variable must be created in advance. One or more CPUE variables can be included.\\
\texttt{bycatch(dat,\ project,\ cpue,\ catch,\ date,\ period,\ names,\ group,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ tran,\ value,\ combine,\ scale,\ output,\ format\_tab)} compares the average CPUE and catch total/share of total catch between one or more species. If including multiple species, \texttt{cpue}, \texttt{catch}, and \texttt{names} should be strings and species should be listed in the same order. \texttt{names} is an optional string of species names that will be used in the plot. If \texttt{names\ =\ NULL}, then species names from catch will be used. \texttt{period} is the time period to aggregate by. Options include \emph{``year''}, \emph{``month''}, and \emph{``weeks''}. \texttt{value} defines whether to return raw catch \emph{(``raw'')} or share of total catch \emph{(``stc'')}.\\
\texttt{trip\_dur\_out(dat,\ project,\ start,\ end,\ units,\ catch,\ hauls,\ group,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ type,\ bins\ =\ 30,\ density,\ scale,\ tran,\ pages,\ remove\_neg,\ output,\ tripID,\ fun.time,\ fun.numeric)} calculates vessel trip duration in the desired unit of time given start and end dates. Obsevations with a negative trip duration can be removed from output using \texttt{remove\_neg\ =\ TRUE}. If data is not at a trip level (each row is a unique trip), specify trip identifiers (\texttt{tripID}) and how to collapse data (\texttt{fun.time} and \texttt{fun.numeric}). You can also calculate CPUE and hauls per unit of time by specifying \texttt{catch} and \texttt{hauls}. Plot output can be a histogram (\texttt{type\ =\ “hist”}) or a frequency polygon (\texttt{type\ =\ "freq\_poly"}). Plot output can be combined on a single page (\texttt{pages\ =\ "single"}) or printed on individual pages (\texttt{pages\ =\ "multi"}).\\
\texttt{density\_plot(dat,\ project,\ var,\ type,\ group,\ date,\ filter\_date,\ date\_value,\ filter\_by,\ filter\_value,\ filter\_expr,\ facet\_by,\ tran,\ scale="fixed",\ bw,\ position)} returns a kernel density estimate, cumulative distribution function, or empirical cdf of selected variable. \texttt{type} options are \emph{``kde''}, \emph{``cdf''}, and \emph{``ecdf''}. The \texttt{group} and \texttt{tran} arguments are not applied to the CDF plot.

\textbf{View relationships}

\texttt{corr\_out(dat,\ project,\ variables,\ ...)} returns the correlation coefficient between numeric variables in \texttt{dat}. \texttt{variables} can be all numeric variables in \texttt{dat} (\texttt{variables\ =\ “all”}) or a list of column names of selected variables. Additional arguments can be added, such as the method to calculate correlation coefficients. The function defaults to displaying the Pearson correlation coefficient. To change the method, specify \texttt{method\ =\ ‘kendall’} or \texttt{method\ =\ ‘spearman’}.\\
\texttt{xy\_plot(dat,\ project,\ var1,\ var2,\ regress\ =\ FALSE)} returns the fitted relationship between two variables. \texttt{var1} and \texttt{var2} are names of variables in \texttt{dat}. \texttt{regress\ =\ TRUE} returns the plot with the fitted linear regression line.

\hypertarget{spatial-functions}{%
\section{Spatial Functions}\label{spatial-functions}}

\hypertarget{mapping-functions}{%
\subsection{Mapping functions}\label{mapping-functions}}

There are several functions to view the data spatially.

\begin{verbatim}
map_kernel()      Kernel density (hotspot).
map_plot()        Static map of haul locations without zones.   
map_viewer()      Interactive map of haul locations or paths with zones.
\end{verbatim}

\hypertarget{creating-and-modifying-data}{%
\section{Creating and Modifying Data}\label{creating-and-modifying-data}}

\hypertarget{data-transformation-and-derivation-functions}{%
\subsubsection{Data transformation and derivation functions}\label{data-transformation-and-derivation-functions}}

Several functions exist that allow users to transform variables or derive new variables and generate specific matrices. These functions are useful to ensure date variables are in the necessary format, to generate rate variables (such as catch/hours fished), calculate more complex variables such as trip distance, and adding variance to confidential data.

Data transformation

\begin{verbatim}
 set_quants()         Coded variable based on the quantiles.
 group_perc()         Within-group percentage.
 group_diff()         Within-group lagged difference.   
 group_cumsum()       Within-group running sum.
\end{verbatim}

Dummy

\begin{verbatim}
 dummy_num()          Vector of TRUE or FALSE, based on condition, the length (rows) of the data. 
 dummy_matrix()       Matrix with same dimensions at the data set filled with TRUE or FALSE.
\end{verbatim}

Nominal ID

\begin{verbatim}
 ID_var()                Generates a nominal variable to indicate distinct hauls or trips.
 create_seasonal_ID()    Generates a fishery season identifier (TRUE/FALSE).
 fleet_table()           Creates and saves a table of fleet conditions for fleet assignment.
 fleet_assign()          Creates a fleet assignment variable based on saved fleet table.
\end{verbatim}

Arithmetic

\begin{verbatim}
 create_var_num()     Defined arithmetic function of two variables.
 cpue()               Catch per unit effort.
\end{verbatim}

Spatial

\begin{verbatim}
assignment_column()   Assign hauls to fishery or regulatory zones.
create_dist_between() Distance between lon/lat points, port, and/or centroid of fishing zone/area.
create_mid_haul()     Returns the midpoint location (lon/lat) for each haul.
create_startingloc()  Returns vector of zone/area at point when choice of where to go next was  
                      made. Used with logit_correction likelihood.
\end{verbatim}

Temporal

\begin{verbatim}
temp_mod()            Transform date variable into desired units (year, month, month/day, minutes).
create_duration()     Duration of time between two temporal variables based on defined time format.
\end{verbatim}

Trip level

\begin{verbatim}
 haul_to_trip()       Collapse the data table from haul to trip.
 trip_distance()      Summed trip distance defined by start and end ports and hauls in between.
 trip_length()        Computes trip duration or hauls per trip. 
\end{verbatim}

All functions require that \texttt{dat}, the name of the primary dataset, be specified. Use quotes if the dataset is pulled from FishSET database. Most functions in this section will add a new variable to the primary dataset. The column name of the new variables is defined with \texttt{name}. The column name defaults to the function name if \texttt{name} is not specified. Spatial functions require a spatial dataset.

\#\#\#Function details

\hypertarget{arithmetic}{%
\paragraph{Arithmetic}\label{arithmetic}}

\texttt{create\_var\_num(dat,\ x,\ y,\ method,\ name)} creates a new numeric variable based on the defined arithmetic expression. \texttt{x} and \texttt{y} are names of numeric variables. \texttt{method} is an arithmetic expression such as addition, subtraction, multiplication, and division.\\
\texttt{cpue(dat,\ xWeight,\ xTime,\ name)} creates the catch per unit effort variable. \texttt{xWeight} is the name of the variable containing the catch data as a weight (pounds, tons, metric tons). \texttt{xTime} is the duration of time and must be in weeks, days, hours, or minutes. Use \texttt{create\_duration()} to transform a date variable into a duration of time variable.
If group-specific cpue is required, cpue must be calculated for each species.

\hypertarget{data-transformation}{%
\paragraph{Data transformation}\label{data-transformation}}

\texttt{group\_perc(dat,\ project,\ id\_group,\ group,\ value,\ name,\ create\_group\_ID,\ drop\_total\_col)} creates a within-group percentage variable using primary (\texttt{id\_group}) and secondary (\texttt{group}) groups. \texttt{value} is the name of a numeric variable to be used to calculate percentages. The \texttt{group} percentage is calculated as the sum of \texttt{value} across \texttt{group} (group\_total) divided by the sum of \texttt{value} across \texttt{id\_group} (total\_group). \texttt{drop\_total\_col} is logical and defines whether or not to add the total\_group and group\_total variables to the dataset.\\
\texttt{group\_diff(dat,\ project,\ group,\ sort\_by,\ value,\ name,\ lag,\ create\_group\_ID,\ drop\_total\_col)} creates a group lagged difference variable within groups. \texttt{value}, a numeric variable, is first summed by the variable(s) in \texttt{group}, then the difference within-group is calculated. \texttt{sort\_by} is a date variable to order \texttt{dat} by. The ``group\_total'' variable gives the total value by group and can be dropped by setting \texttt{drop\_total\_col\ =\ TRUE}. A group ID column can be created using the variables in \texttt{group} by setting \texttt{create\_group\_ID\ =\ TRUE}.\\
\texttt{group\_cumsum(dat,\ project,\ group,\ sort\_by,\ value,\ name,\ create\_group\_ID,\ drop\_total\_col)} sums \texttt{value} by group, then cumulatively sums within \texttt{group}. For example, a running sum by trip can be made by entering variables that identify unique vessels and trips into \texttt{group} and a numeric variable (such as catch or number of hauls) into \texttt{value}. The group total variable can be dropped by setting \texttt{drop\_total\_col\ =\ TRUE}. A group ID column can be created using the variables in group by setting \texttt{create\_group\_ID\ =\ TRUE}.\\
\texttt{set\_quants(dat,\ x,\ quant.cat\ =\ c(0.2,\ 0.25,\ 0.4),\ custom.quant,\ name)} transform \texttt{x} into a quantile category using pre-defined (\texttt{quant.cat}) or user-defined (\texttt{custom.quant}) quantiles.

\begin{description}
\item[Predefined quantiles]
0.1 (0\%, 10\%, 20\%, 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, 100\%)\\
0.2 (0\%, 20\%, 40\%, 60\%, 80\%, 100\%)\\
0.25 (0\%, 25\%, 50\%, 75\%, 100\%)\\
0.4 (0\%, 10\%, 50\%, 90\%, 100\%)
\end{description}

\hypertarget{dummy}{%
\paragraph{Dummy}\label{dummy}}

\texttt{dummy\_num(dat,\ var,\ value,\ opts,\ name)} creates a binary variable contrasting a variable (\texttt{var}) based on a splitting value (\texttt{value}) and splitting rule (\texttt{opts}). The function is used to contrast before/after a policy is implemented or zone closures. \texttt{value} should be a year if \texttt{var} is a date variable, a factor level if \texttt{var} is a factor variable, a single or range of numbers if \texttt{var} is a numeric variable. \texttt{Opts} may be \emph{``x\_y''} or \emph{``more\_less''}. \emph{``x\_y''} sets each element of \texttt{var} to 1 if the element matches \texttt{value}, otherwise 0. For \emph{``more\_less''}, each element of \texttt{var} less than \texttt{value} is set to 0 and all elements greater than \texttt{value} are set to 1. If \texttt{var} is a factor, then elements that match \texttt{value} will be set to 1 and all other elements set to 0. Default is set to \emph{``more\_less''}.\\
\texttt{dummy\_matrix(dat,\ x)} creates a dummy matrix of TRUE/FALSE based on values of variable \texttt{x} with dimensions \emph{(number of observations in dataset)} (number of factors in x)* where each column is a unique factor level of \texttt{x}. Values are TRUE if the value in the column matches the column factor level and FALSE otherwise.

\hypertarget{nominal-id}{%
\paragraph{Nominal ID}\label{nominal-id}}

\texttt{ID\_var(dat,\ vars,\ name,\ type,\ drop)} is used to create a haul or trip identifier variable from on or more variables (\texttt{vars}). \texttt{type} is the class type of the new ID column. Choices are \emph{``string''} and \emph{``integer''}. \emph{``string''} returns a character vector where each column in \texttt{vars} is combined and separated with an underscore "\_". \emph{``integer''} returns an integer vector where each value corresponds to a unique group in \texttt{vars}. Set \texttt{drop\ =\ TRUE} to drop \texttt{vars} columns from \texttt{dat}.\\
\texttt{create\_seasonal\_ID(dat,\ seasonal.dat,\ use.location,\ use.geartype,\ sp.col,\ target)} uses a table of fishery season dates to create one or more fishery season identifier variables. The function matches fishery season dates provided in \texttt{seasonal.dat} to the first date variable in \texttt{dat}. If fishery season dates vary by location or gear type, set \texttt{use.location\ =\ TRUE} and \texttt{use.geartype\ =\ TRUE}. Output is a seasonID variable and/or multiple \emph{seasonID*fishery} variables. The seasonID variable is a vector of fisheries. If a target fishery (\texttt{target}) is not defined, each row is the first observed fishery in \texttt{seasonal.dat} for which the fishery season dates encompasses date in \texttt{dat}. If target fishery is defined, then SeasonID is defined by whether the target fishery is open on the date in \texttt{dat} or a different fishery. The vector is filled with \emph{``target''} or \emph{``other''}. \emph{SeasonID*fishery} variables are a TRUE/FALSE seasonID vector for each fishery (labeled by seasonID and fishery) where TRUE indicates the dates for a given row in the main data table fall within the fishery dates for that fishery.

\hypertarget{spatial}{%
\paragraph{Spatial}\label{spatial}}

\texttt{assignment\_column(dat,\ project,\ gridfile,\ lon.dat,\ lat.dat,\ cat,\ closest.pt\ =\ FALSE,\ lon.grid\ =\ NULL,\ lat.grid\ =\ NULL,\ hull.polygon\ =\ FALSE,\ epsg\ =\ NULL)} assigns observations (hauls) to zones. \texttt{gridfile} is the spatial dataset defining zone boundaries, \texttt{lon.dat} and \texttt{lat.dat} are variables in \texttt{dat} used to assign each observation to a zone. \texttt{cat} is a variable or list in \texttt{gridfile} that identifies the individual areas or zones. \texttt{lon.grid} and \texttt{lat.grid}are properties or lists from \texttt{gridfile} containing longitude and latitude data. These data are required if \texttt{gridfile} is not class \emph{sf}. \texttt{hull.polygon} and \texttt{closest.pt} are logical optional arguments. Set \texttt{hull.polygon\ =\ TRUE} if spatial data creating polygons are sparse or irregular. To assign observations that fall outside any zones to the closest zone polygon set \texttt{closest.pt\ =\ TRUE}. \texttt{epsg} is the spatial reference system. Specify \href{http://spatialreference.org}{\texttt{epsg}} if the projected coordinate systems do not match.\\
\texttt{create\_dist\_between(dat,\ start,\ end,\ units,\ name)} adds a distance between two points variable to \texttt{dat}. \texttt{start} and \texttt{end} locations define how to find the lon/lat of starting and ending locations. Options are a port (specify port variable name), observed latitude and longitude location (specify variable name containing longitude data then latitude data as a character string), or the centroid of the observed fishery zone location (specify `centroid'). Distance is returned in the requested \texttt{units} (\emph{``miles'', ``kilometers'', ``midpoint''}).
Additional arguments may be required, depending upon the \texttt{start} and \texttt{end} arguments. These are added through prompts.

\begin{description}
\item[Additional arguments]
\emph{Port arguments}\\
portTable - Table contains the latitude and longitude of ports.

\emph{Centroids arguments}\\
gridfile - Spatial data file.\\
lon.dat - Longitude variable from dat.\\
lat.dat - Latitude variable from dat.\\
lon.grid - Property or variable from gridfile containing longitude data.\\
lat.grid - Property or variable from gridfile containing latitude data.\\
cat - Property or variable from gridfile that identifies the individual areas or zones.\\
\end{description}

\texttt{create\_mid\_haul(dat,\ start=c("lon",\ "lat"),\ end=c("lon",\ "lat"),\ name)} returns the latitude and longitude of the midpoint location of each haul. Each row of \texttt{dat} must be a unique haul. Requires a \texttt{start} and \texttt{end} point for each observation. \texttt{start} and \texttt{end} must be specified in the order of longitude then latitude.\\

\texttt{create\_startingloc(dat,\ gridfile,\ portTable,\ trip\_id,\ haul\_order,\ starting\_port,\ lon.dat,\ lat.dat,\ cat,\ name,\ lon.grid,\ lat.grid)} creates a variable containing the zone/area location of a vessel when choice of where to fish next was made. This variable is required for the full information model with Dahl's correction (logit\_correction). Unique trips in \texttt{dat} are identified with \texttt{trip\_id}. \texttt{haul\_order} identifies the order of hauls within \texttt{trip\_id}. Generally, the first zone of a trip is the departure port and is identified with \texttt{starting\_port}. \texttt{gridfile} and \texttt{portTable} are additional data tables containing information on zone boundaries and port locations, respectively. These two data tables along with \texttt{lon.dat}, \texttt{lat.dat}, \texttt{cat}, \texttt{lon.grid}, and \texttt{lat.grid} are called by the \texttt{assignment\_column()} function to assign starting port locations and haul locations to zones. \texttt{lon.grid} and \texttt{lat.grid} are required if \texttt{gridfile} is a data frame or list.

\hypertarget{temporal}{%
\paragraph{Temporal}\label{temporal}}

\texttt{temporal\_mod(dat,\ x,\ define.format,\ name)} extracts a time unit from \texttt{x}, date variable in \texttt{dat}, in the desired format (\texttt{define.format}). The FishSET function \texttt{date\_parser()} is first called to convert the date variable into recognized class if required. The \texttt{as.Date()} function from the \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.Date}{base package} is then called to extract the time in the desired format. \texttt{define.format} can be a predefined format (see below) or \href{https://www.stat.berkeley.edu/~s133/dates.html}{user-defined}.

Predefined formats:

\begin{verbatim}
Value    as.Date format      Output   
_______________________________________________________________
year    "%Y"                 year
month   "%Y/%m"              year and month
day     "%Y/%m/%d"           year, month, and day
hour    "%Y/%m/%d %H"        year, month, day and hour
minute  “%Y/%m/%d %H:%M"     year, month, day, hour, and minute
\end{verbatim}

\texttt{create\_duration(dat,\ start,\ end,\ units=c("week",\ "day",\ "hour",\ "minute"),\ name)} returns the duration of time, in specified \texttt{units}, between two temporal variables (\texttt{start}, \texttt{end}) in \texttt{dat}. A duration of time variable is required for other functions, such as \texttt{cpue()}.

\hypertarget{trip-level}{%
\paragraph{Trip-level}\label{trip-level}}

\texttt{haul\_to\_trip(dat,\ project,\ fun.numeric,\ fun.time,\ tripID)} collapses \texttt{dat} from haul to trip level. Unique trips are identified by \texttt{tripID}. \texttt{tripID} may be one or more variables from \texttt{dat}. \texttt{fun.numeric} and \texttt{fun.time} define how multiple observations for a trip are collapsed. Options are \emph{min}, \emph{mean}, \emph{max}, and \emph{sum} for numeric variables and \emph{min}, \emph{mean}, and \emph{max} for temporal variables. For variables that are not numeric or dates, the first observation is used.\\
\texttt{create\_trip\_distance(dat,\ PortTable,\ trip\_id,\ starting\_port,\ starting\_haul\ =\ c("Lon",\ "Lat"),\ ending\_haul\ =\ c("Lon",\ "Lat"),\ ending\_port,\ haul\_order,\ name,\ a\ =\ 6378137,\ f\ =\ 1/298.257223563)} sums distance across a trip based on starting and ending ports and hauls in between. Inputs are the trips, ports, and hauls from the primary dataset, and the latitude and longitude of ports from the \texttt{PortTable}. The ellipsoid arguments, \texttt{a} and \texttt{f}, can be changed if an ellipsoid other than WGS84 is appropriate. See the \href{https://cran.r-project.org/web/packages/geosphere/geosphere.pdf}{geosphere R package}.\\
\texttt{create\_trip\_centroid(dat,\ lon,\ lat,\ weight.var,\ ...)} returns the centroid, in longitude and latitude, for each trip. Specify a weighting variable (\texttt{weight.var}) to calculate the weighted centroid. Additional arguments can be added that define unique trips. If no additional arguments are added, each row will be treated as a unique trip.

\hypertarget{modeling-functions}{%
\section{Modeling Functions}\label{modeling-functions}}

\hypertarget{modeling-functions-1}{%
\subsection{Modeling functions}\label{modeling-functions-1}}

Model development in FishSET requires preparing the data and defining model parameters. This section details the required and optional data, model choices, and model output evaluation functions.

Generate necessary data

\begin{verbatim}
create_startingloc()        Starting location variable.         
create_alternative_choice() Define alternative fishing choices.   
create_expections()         Define expected catch/revenue matrix.   
\end{verbatim}

Check data

\begin{verbatim}
check_model_data()          Check model data for possible issues with data.
\end{verbatim}

Design model

\begin{verbatim}
make_model_design()         Make model design.                      
discretefish_subroutine()   Run model.                              
\end{verbatim}

Assess model output

\begin{verbatim}
globalcheck_view()          Model error.
model_out_view()            Model output.   
Metrics model_fit()         Model comparison.
select_model()              Select best model.  
\end{verbatim}

\#\#\#Generate necessary data
All models require catch data, data on the chosen fishing location (choice), and a distance matrix. Price data, location when choice of where to fish next, and expected catch matrices are required for some models. Additional optional data that interact with travel distance or vary by location may also be added. These data and the functions to generate them are outlined next.

\textbf{Distance matrix}\\
This function returns the matrix of distances between observed and alternative fishing choices (where they could have fished but did not). The distance matrix is saved in the model design file in the FishSET database and pulled when the model run function is called.

\begin{verbatim}
create_alternative_choice(dat, project, alt_var, occasion, griddedDat, dist.unit, min.haul, gridfile, cat, lon.dat, lat.dat, hull.polygon, closest.pt, lon.grid, lat.grid, weight.var)
\end{verbatim}

The distance matrix can be generated from the primary haul-level data (\texttt{dat}) or from gridded data (\texttt{griddedDat}) such as sea surface temperature. We describe generating the distance matrix from \texttt{dat} first.

\emph{From \texttt{dat}}\\
Define how to find the longitude and latitude for the starting (\texttt{occasion}) and alternative (\texttt{alt\_var}) locations. Choices are the centroid of the zone where the haul occurred (\emph{``centroid''}), a port variable in \texttt{dat} (such as disembarked port), or paired latitude and longitude variables in \texttt{dat} such as haul starting location. Distance can be returned in \emph{``miles''}, \emph{``kilometers''}, or \emph{``meters''} (\texttt{dist.unit}). Remove zones with insufficient data by setting \texttt{min.haul} to the minimum number of hauls required.

If \texttt{alt\_var} or \texttt{occasion} are \emph{``centroid''} then the centroid of zones must be obtained. If a centroid table exists in the FishSET database and a zone identifier variables is in \texttt{dat}, then only \texttt{gridfile} needs to be defined. \texttt{gridfile} should the name of the gridfile, in quotes. For example, \texttt{gridfile\ =\ "gridname"}. If centroids or zone assignments must be calculated then \texttt{gridfile}, \texttt{cat}, \texttt{lon.dat}, and \texttt{lat.dat} must be specified. \texttt{hull.polygon}, \texttt{closest.pt}, \texttt{weight.var} are optional \texttt{lon.grid} and \texttt{lat.grid} must be defined if \texttt{gridfile} is a data frame. See \texttt{assignment\_column()} for more details on optional arguments.

If \texttt{occasion} is a port variable, a port table containing the latitude and longitude of ports is required. If a port table does not exist in the FishSET database run the \texttt{load\_port()} function before running \texttt{create\_alternative\_choice}.

If \texttt{alt\_var} or \texttt{occasion} are paired longitude and latitude variables from \texttt{dat} then \texttt{alt\_var} or \texttt{occasion} should be specified as \emph{\texttt{c(lon,\ lat)}}.\\
\emph{From \texttt{griddedDat}}\\
Alternatively, the distance matrix can be generated from a gridded dataset, such as sea surface temperature. In this case, only \texttt{dat}, \texttt{project}, and \texttt{griddedDat} must be specified. Columns in the gridded data file must be individual zones and must match the zone identifier variable in \texttt{dat}. The gridded data can vary over a second dimension, such as date, but must match a variable in \texttt{dat}. We recommend saving the gridded data file to the FishSET database using the \texttt{load\_grid()} function.

\textbf{Expected catch matrix}\\
This function returns the expected catch or revenue matrix for alternative fishing zones (zones they could have chosen but that the fisher did not in this choice opportunity). This matrix is required to run the conditional logit model that utilizes an average revenue specification.

\begin{verbatim}
create_expectations(dat, project, catch, price, defineGroup, temp.var, temporal, calc.method, lag.method, empty.catch, empty.expectation, temp.window, temp.lag, year.lag, dummy.exp, replace.output)
\end{verbatim}

The \texttt{create\_expectations()} function requires a catch variable. To return expected revenue, include a price or value variable from \texttt{dat}. \texttt{price} is multiplied against catch to generate revenue. If revenue exists in \texttt{dat} and you wish to use this revenue instead of price, then \texttt{catch} must be a vector of 1 of length equal to \texttt{dat}.

Expected catch/revenue can be calculated across the entire dataset (\texttt{defineGroup\ =\ “fleet”}) or within groups and across the entire record of catch (\texttt{temp.var\ =\ NULL}) or can take variation in catch over time into account. To take temporal patterns in catch into account, first identify the temporal variable (\texttt{temp.var}) in \texttt{dat} to use. Next, select whether to use a sequential (\texttt{temporal\ =\ "sequential"}) or daily timeline (\texttt{temporal\ =\ "daily"}). Sequential timeline sorts data by date but does not take into account that days may be missing. Daily timeline adds in these dates with NA (missing value). With sequential timeline, a window size of seven means catch would be the average over the past seven fishing days. Whereas, with daily timeline, it would be the average over seven calendar days. \texttt{year.lag} can be 0 (current year), 1 (previous year's data), or any other viable years to go back. Window size (\texttt{temp.window}) is in days and can be current day (0), a week (7), or any other numeric value. \texttt{temp.lag} is numeric and in days. For each day \emph{x} in \texttt{temp.var}, catch is the average catch across \emph{w} days (\texttt{temp.window}) starting \emph{d} days (\texttt{temp.lag}) and \emph{y} years (\texttt{year.lag}) before \emph{x}.

Using the specified moving window parameters, a matrix of average catch is created with \emph{zone*group} creating rows and \emph{\texttt{date}} the columns. This is the standard average catch \texttt{(calc.method\ =\ “standardAverage”}). Alternatively, you can use the simple lag regression of the mean (\texttt{calc.method\ =\ “simpleLag”}) which calculates the predicted value for each zone\texttt{*}group and date given regression coefficients \emph{p} for each zone\texttt{*}group. The lag method (\texttt{lag.method}), can use regression over entire group (\emph{``simple''}) or for grouped time periods (\emph{``grouped''}).

The expected catch matrix is pulled from the matrix of calculated catches for each date and zone. The matrix is of dimensions \emph{(number of rows of dat)*(number of alternatives)}. Expected catch is filled out by mapping the calculated catch for each zone given the observed date (if specified) and group (if specified) in \texttt{dat}. Note that empty catch values are considered to be times of no fishing activity and are not included. Values of 0 in the catch variable are considered times when fishing activity occurred but with no catch. These zero values are included in calculations. Sparsity in data should be considered when deciding how to take into account that catch may vary over time. Check for sparsity with the \texttt{temp\_obs\_table()} function. A broader window size or using the entire temporal record may be necessary. Empty catch values and empty expected catch values can be filled but only on a limited basis as doing so can lead to biased or misleading results. If there are a lot of empty values, consider changing the temporal arguments to reduce data sparsity. \texttt{empty.catch} can be \emph{0}, the average of all catch values (\emph{``allCatch''}") or the average of grouped catch values (\emph{``groupedCatch''}). \texttt{empty.expectation} can be 0 or 1e-04.

The function returns four expected catch or expected revenue matrices based on predefined window size and lags in days and years and all other user-defined arguments :

\begin{verbatim}
selected temporal arguments
expected catch/revenue based the previous two days (short-term) catch,
expected catch/revenue based the previous seven days (medium-term) catch,
and expected catch/revenue based on the previous years (long-term) catch.
\end{verbatim}

\textbf{Starting location}\\
The starting location vector is required for the full information model with Dahl's correction (\texttt{logit\_correction}). The vector is the zone location of a vessel when the decision of where to fish next was made. Generally, the first zone of a trip is the departure port. Create the starting location vector before creating the model design file (\texttt{make\_model\_design()}).

\begin{verbatim}
create_startingloc(dat, gridfile, portTable, trip_id, haul_order, starting_port, lon.dat, lat.dat, cat, name = "startingloc", lon.grid, lat.grid)
\end{verbatim}

The \texttt{create\_startingloc()} function adds a starting location vector to the dataset (\texttt{dat}).

A zonal identifier variable is required. If zone identifier variable does not exist, the \texttt{assignment\_column()} function is called to assign starting port locations and haul locations to zones and the following arguments are required: \texttt{gridfile}, \texttt{lon.dat}, \texttt{lat.dat}, \texttt{cat}, \texttt{lon.grid}, \texttt{lat.grid}. See \texttt{assignment\_column} documentation for details.

The first zone is generally the zone of the departure port. The table containing port latitude and longitudes (\texttt{portTable}) is required. This should be in the FishSET database. Also required is the variable in \texttt{dat} containing names of starting ports (\texttt{starting\_port}). \texttt{trip\_id} is a variable in \texttt{dat} containing unique trip identifiers. \texttt{haul\_order} is a variable in \texttt{dat} containing the order of hauls within trips.

\textbf{Additional optional data}\\
\texttt{vars1} is a character string of additional \emph{travel-distance} variables to include in the model, such as vessel characteristics. \texttt{vars2} is a character string of additional variables to include in the model, wind speed within zones. These depend on the likelihood. The details of the functions depend on the likelihood function and are outlined in the \href{LINK1!}{FishSET Help Manual} and likelihood function documentation.

\#\#\#Model choices
Model choices include the likelihood functions, optimization method, optimization options, and starting parameter values. These, along with the required and optional data are specified in the \texttt{model\_design\_function()}. The model design file is saved to the FishSET database and called by the \texttt{discretefish\_subroutine()} function to run the models.

\begin{verbatim}
make_model_design(dat, project, catchID, replace = TRUE, likelihood = NULL, initparams, optimOpt, methodname, mod.name, vars1 = NULL, vars2 = NULL, priceCol = NULL, startloc = NULL, polyn = NULL)

discretefish_subroutine(project, select.model = FALSE)
\end{verbatim}

The model design function requires the variable name in \texttt{dat} containing catch data. The distance and expected catch/revenue matrices do not have to be specified. They will be pulled based on \texttt{project} from the FishSET database. \texttt{startloc} is the name of the variable containing starting locations, if required. \texttt{priceCol} is optional argument to specify the variable in \texttt{dat} containing price data. It is required for the expected profit models (\emph{epm\_normal, epm\_weibull, epm\_lognormal}). \texttt{vars1} is a character string of optional \emph{travel-distance} variables and \texttt{vars2} is a character string of optional \emph{grid-varying} variables. These two arguments should be \emph{NULL} if no additional data is being included. \texttt{polyn} is the correction polynomial degree. It is required for the \emph{logit\_correction} model.

FishSET allows only one model design file per project. Multiple models can be added to this file. Models are added on at a time using the \texttt{make\_model\_design()} function. Identify individual models with \texttt{model.name}. Models are added to the model design file by setting \texttt{replace\ =\ FALSE}. \texttt{replace\ =\ TRUE} will remove the existing model design file and only the currently specified model in \texttt{make\_model\_design()} will be in the model design file.

The remaining arguments in \texttt{make\_model\_design()} are detailed below.

\textbf{Likelihood functions (\texttt{likelihood})}

FishSET has six built-in likelihood functions.

\begin{verbatim}
Function            Likelihood name                                         Reference
________________________________________________________________________________________________
logit_c             Conditional logit likelihood                              McFadden 1974
logit_avgcat        Average catch multinomial logit procedure                 Abbot and Wilen 2011
logit_correction    Full information model with Dahl’s correction function    Dahl 2002 
epm_normal          Expected profit model with normal catch function          Haynie and Layton 2010
epm_weibull         Expected profit model with Weibull catch function         Haynie and Layton 2010
epm_lognormal       Expected profit model with lognormal catch function       Haynie and Layton 2010
\end{verbatim}

See function documentation or the \href{LINK!!!}{Help Manual} for more details.

\textbf{Initial parameter values (\texttt{initparams})}

The number of initial parameter values and the order depend upon the specified likelihood function and the number of travel-distance and grid-varying variables included.

\emph{logit\_c}\\
Starting parameter values take the order of \texttt{c{[}(alternative-specific\ parameters),\ (travel-distance\ parameters){]}}. The length is the \emph{number of alternative-specific variables} plus \emph{number of travel-distance variables}.

\emph{logit\_avgcat}\\
Starting parameter values take the order of: \texttt{c{[}(average-catch\ parameters),\ (travel-distance\ parameters){]}}. The length is the \emph{(number of average-catch variables)*(k-1)} plus the \emph{number of travel-distance variables}. \texttt{k} equals the number of alternative fishing choices.

\emph{logit\_correction}\\
starting parameter order takes: \texttt{c{[}(marginal\ utility\ from\ catch),\ (catch-function\ parameters),\ (polynomial\ starting\ parameters),\ (travel-distance\ parameters),\ (catch\ sigma){]}}. The marginal utility from catch and catch sigma are of length equal to unity respectively. The catch-function and travel-distance parameters are of length \emph{(number of catch variables)*(k)} and \emph{number of cost variables} respectively. The number of polynomial interaction terms is currently set to 2, so given the chosen degree \texttt{polyn} there should be \emph{(((polyn+1)*2)+2)*(k)} polynomial starting parameters, where \texttt{k} equals the number of alternative fishing choices.

\emph{epm\_normal}\\
Starting parameters values take the order of: \texttt{c{[}(catch-function\ parameters),\ (travel-distance\ parameters),\ (catch\ sigma(s)),\ (scale\ parameter){]}}. The catch-function and travel-distance parameters are of length \emph{(number of catch-function variables)*(k)} and \emph{(number of travel-distance variables)} respectively, where \texttt{k} equals the number of alternative fishing choices. The catch sigma(s) are either of length equal to unity or length \texttt{k} if estimating location-specific catch sigma parameters. The scale parameter is of length equal to unity.

\emph{epm\_weibull}\\
Starting parameter values takes the order of: \texttt{c{[}(catch-function\ parameters),\ (travel-distance\ parameters),\ (catch\ sigma(s)),\ (scale\ parameter){]}}. The catch-function and travel-distance parameters are of length \emph{(number of catch-function variables)*(k)} and \emph{(number of travel-distance variables)} respectively, where \texttt{k} equals the number of alternative fishing choices. The catch sigma(s) are either of length equal to unity or length \texttt{k} if estimating location-specific catch sigma parameters. The scale parameter is of length equal to unity.

\emph{epm\_lognormal}\\
Starting parameter values takes the order of: \texttt{c{[}(catch-function\ parameters),\ (travel-distance\ parameters),\ (catch\ sigma(s)),\ (scale\ parameter){]}}. The catch-function and travel-distance parameters are of length \emph{(number of catch-function variables)*(k)} and \emph{(number of travel-distance variables)} respectively, where \texttt{k} equals the number of alternative fishing choices. The catch sigma(s) are either of length equal to unity or length \texttt{k} if estimating location-specific catch sigma parameters. The scale parameter is of length equal to unity.

\begin{description}
\item[Optimization options (\texttt{optimOpt})]
Maximum number of iterations\\
Relative convergence tolerance\\
Report frequency\\
Level of tracing information returned
\end{description}

As a guide, the optimization function \texttt{optim} in the \textbf{stats} package defaults to 100 for derivative-based methods, 500 for ``Nelder-Mead'' and 10000 for ``SANN''. The relative tolerance should be very small. The default is 1e-08. Report frequency is the frequency of reports such as every 10 iterations for ``BFGS'' or every 100 temperatures for ``SANN''. Tracing must be 1 or higher for reports to be returned. Higher values return more details.

\textbf{Optimization method (\texttt{methodname})}\\
Choices are ``BFGS'', ``L-BFGS-B'', ``Nelder-Mead'', ``Brent'', ``CG'', and ``SANN''. Default is ``BFGS''.

\textbf{Running and saving models}
To run the defined models, use \texttt{discretefish\_subroutine(project,\ select.model\ =\ FALSE)}. Each model can take 10 or more minutes to run.

It may be desirable to identify the ``best'' or preferred models for future reference. Identifying the ``best'' model can be done in the \texttt{discretefish\_subroutine()} function by setting \texttt{select.model\ =\ TRUE} or using the \texttt{select\_model(project,\ overwrite\_table=FALSE)} function. Both produce a \emph{modelChosen} table that is saved to the FishSET database. This table is not used in any functions.
R Console

In the R console, an interactive data table can be opened by setting select.model in discretefish\_subroutine() to TRUE or with the select\_model() function.
\texttt{select\_model()}
The interactive table contains the name of the model and model measures of fit for each model. Users can delete models from the table and select the preferred model by checking the selected box. The table is saved to the FishSET database with two new columns added, a TRUE/FALSE selected column and the date it was selected if overwrite\_table is TRUE. The table is saved with the phrase `modelChosen' in the FishSET database.
FishSET GUI
Identifying and recording the ``best'' or preferred model is done by checking the Selected box next to the desired model in the Measures of Fit table in the Model Comparison subtab. Click the blue Save Table button to record this selection. Use the Delete Row button to remove a model.

\#\#\#Model output evaluation
Output from the model is saved to the FishSET database in three tables. These tables are saved based on the project name, table phrase, and the date the model was run in YYMD format. The model output tables all contain the phrase \emph{modelout} and the error output tables all contain \emph{ldglobalcheck}.

The first table contains the initial global log likelihood, initial log likelihood for alternative choices, and starting parameters. Use these for assessing the cause of model errors. The second table contains information on the model output, such as convergence, standard errors, and the inverse Hessian matrix. The model comparison or measure of fit table contains AIC, AIC\textsubscript{c}, BIC, Pseudo R\textsuperscript{2}.

\hypertarget{policy-and-welfare-analysis}{%
\section{Policy and Welfare Analysis}\label{policy-and-welfare-analysis}}

\hypertarget{reporting}{%
\section{Reporting}\label{reporting}}

FishSET includes a report template. It is written in RMarkdown and allows for output from FishSET to be easily incorporated into a shareable report. The template is divided into sections, each with a series of guiding questions and suggestions for your analysis. These questions will prompt you to explain how important aspects of your analysis were conducted, such as how the data was prepared, how models were defined, and which plots and tables to include. This is a suggested report structure. Sections and subsections can be added, deleted, or moved.

The report template is called \texttt{report\_tempate.Rmd} and is available in the \emph{report} folder of the FishSET package. Open the file and choose \emph{Save as\ldots{}} in the \textbf{File} tab in RStudio. Save the template to a meaningful name. This can be done for each project.
The template contains guidance on working in RMarkdown and examples of script.

\end{document}
