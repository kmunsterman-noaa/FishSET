---
title: "FishSET Simulation"
author: "Paul Carvalho"
format: pdf
editor: visual
---

## FishSET Simulation

This is a simulated example of FishSET to review and understand modeling principles. We can also use this simulated dataset to test modeling assumptions.

## 1. Setup and Parameters

```{r}
#| label: setup
#| message: false
#| warning: false

library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(RTMB)
library(sf)
library(Matrix)
library(viridis)
library(MASS)
library(spdep)

set.seed(42)
```

### Spatial grids

Underlying "true" spatial density of revenue.

```{r}
#| label: create-spatial-grid

# Define the domain
domain <- st_polygon(list(cbind(c(0,100,100,0,0), c(0,0,100,100,0))))
grid_sf <- st_make_grid(domain, n = c(10,10))
n_zones <- length(grid_sf)

# Get centroids for distance/utility calcs
grid_centroids <- st_centroid(grid_sf)
coords <- st_coordinates(grid_centroids)

# Simulate spatially autocorrelated revenue
mu <- c(70, 60) # Center of the hotspot
sigma <- diag(2) * 25^2

# Generate density
density_vals <- dmvnorm(coords, mu = mu, Sigma = sigma)

# Scale density to revenue ($)
norm_density <- (density_vals - min(density_vals)) / (max(density_vals) - min(density_vals))
base_revenue <- 500 + (norm_density * 4500)

# Create the zones dataframe
zones <- data.frame(
  zone_id = 1:n_zones,
  base_revenue = base_revenue,
  x = coords[,1],
  y = coords[,2]
)

# Visualize "truth"
plot_data <- st_sf(grid_sf, revenue = base_revenue)
ggplot(plot_data) +
  geom_sf(aes(fill = revenue)) +
  scale_fill_viridis_c(option = "plasma") +
  ggtitle("Revenue (Resource Density)") +
  theme_minimal()
```

```{r}
#| label: create-hidden-error

mu_error <- c(30, 40)
sigma_error <- diag(2) * 40^2 # Broad spatial correlation

# Generate the field
error_vals <- dmvnorm(coords, mu = mu_error, Sigma = sigma_error)

# Normalize to a reasonable magnitude for Utility
norm_error <- (error_vals - min(error_vals)) / (max(error_vals) - min(error_vals))
spatial_error_term <- (norm_error * 2.0) - 1.0 # Ranges from -1 to +1

# Add to zones dataframe
zones <- zones %>%
  mutate(unobserved_util = spatial_error_term)

# Visualize the hidden error
ggplot(st_sf(grid_sf, val = spatial_error_term)) +
  geom_sf(aes(fill = val)) +
  scale_fill_viridis_c(option = "plasma") +
  ggtitle("The Hidden Spatial Error Term")
```

```{r}
#| label: calculate-spatial-distances

zone_dist_matrix <- as.matrix(st_distance(grid_centroids, grid_centroids))

# Convert to numeric (strip unit class) for the simulation loop
zone_dist_matrix <- apply(zone_dist_matrix, 2, as.numeric)
```

## 2. Simulating the Data

### Trip and Haul Data

```{r}
#| label: create-trips-hauls

n_trips <- 500
base_fuel_cost <- 150
fuel_cost_per_km <- 0.8

# Create n_trips, each with a departure port
trips <- data.frame(trip_id = 1:n_trips)

# Create hauls: each trip gets 2 to 6 hauls
hauls_per_trip <- sample(2:6, n_trips, replace = TRUE)
hauls <- data.frame(trip_id = trips[rep(1:n_trips, times = hauls_per_trip),])

# Create a unique haul_id and sequence number
hauls$haul_id <- 1:nrow(hauls)
hauls <- hauls %>%
  group_by(trip_id) %>%
  mutate(haul_num_in_trip = row_number()) %>%
  ungroup()

# Show the first few rows of the hauls data
cat("Total choice situations (hauls):", nrow(hauls), "\n")
head(hauls, 8)
```

### Simulation of Choices

This is the core simulation logic. We loop through each trip and each haul *in order*, calculating dynamic distances and simulating choices one by one.

```{r}
#| label: sequential-simulation
#| echo: False

# True parameter values
beta_revenue <- 0.001
beta_fuel_cost <- -0.05

# Initialize lists to store results
all_choices_long_list <- list() # Store the full long-format data
all_chosen_hauls_list <- list() # Store only the chosen rows

# Get vector of unique trip IDs
unique_trips <- unique(hauls$trip_id)

# Loop over each TRIP
for (i in seq_along(unique_trips)) {
  current_trip_id <- unique_trips[i]
  
  # Get all hauls for this trip, in order
  hauls_in_trip <- hauls %>% 
    filter(trip_id == current_trip_id) %>%
    arrange(haul_num_in_trip)
  
  previous_chosen_zone <- NA # Reset for each new trip
  
  # Loop over each HAUL within the trip
  for (h in 1:nrow(hauls_in_trip)) {
    current_haul_id <- hauls_in_trip$haul_id[h]
    current_haul_num <- hauls_in_trip$haul_num_in_trip[h]
    
    # Create the choice set for this haul (one row per zone)
    choice_set_df <- zones %>%
      mutate(haul_id = current_haul_id)
    
    # Calculate Dynamic Distance
    if (current_haul_num == 1) {
      # Haul 1: Distance is from Departure Port (grid cell 1)
      choice_set_df$distance <- zone_dist_matrix[1, ]
    } else {
      # Haul > 1: Distance is from previous chosen zone
      choice_set_df$distance <- zone_dist_matrix[previous_chosen_zone, ]
    }
    
    # Calculate covariates and add noise to the data
    choice_set_df <- choice_set_df %>%
      mutate(
        revenue = base_revenue + rnorm(n(), 0, 50),
        fuel_cost = base_fuel_cost + (distance * fuel_cost_per_km)
      )
    
    # Calculate utility (V) - "TRUE" V
    choice_set_df$V <- (
      beta_revenue * choice_set_df$revenue +
      beta_fuel_cost * choice_set_df$fuel_cost +
      (3 * choice_set_df$unobserved_util) # hidden from logit models
    )
    
    # Simulate Choice
    # softmax function to calculate probabilities
    choice_set_df$prob <- (exp(choice_set_df$V)) / (sum(exp(choice_set_df$V)))
    
    # Sample one zone per haul, weighted by its probability
    chosen_row <- choice_set_df %>% sample_n(1, weight = prob)
    
    previous_chosen_zone <- chosen_row$zone_id # Update for next haul
    
    # Store Results
    # Add 'choice' column (0/1)
    choice_set_df$choice <- ifelse(choice_set_df$zone_id == previous_chosen_zone, 1, 0)
    all_choices_long_list[[length(all_choices_long_list) + 1]] <- choice_set_df
    all_chosen_hauls_list[[length(all_chosen_hauls_list) + 1]] <- chosen_row
  }
}

# Bind all data frames
df_long <- bind_rows(all_choices_long_list)
df_chosen <- bind_rows(all_chosen_hauls_list)

print(head(df_long, 12))

```

### Visualize effort

```{r}
#| label: viz-choices

visits <- df_chosen %>% count(zone_id)

# Merge back to spatial grid
results_sf <- grid_sf %>%
  st_as_sf() %>%
  mutate(zone_id = 1:n()) %>%
  left_join(visits, by = "zone_id") %>%
  mutate(n = replace_na(n, 0))

ggplot(results_sf) +
  geom_sf(aes(fill = n)) +
  scale_fill_viridis_c(option = "plasma") +
  ggtitle("Simulated Fishing Effort Distribution") +
  theme_minimal()

ggplot(plot_data) +
  geom_sf(aes(fill = revenue)) +
  scale_fill_viridis_c(option = "plasma") +
  ggtitle("True Spatially Autocorrelated Revenue (Resource Density)") +
  theme_minimal()

```

## 3. Reformat Data for NLL Function

```{r}
#| label: cond-logit-design

# Create design matrix
X_mat <- model.matrix(~ revenue + fuel_cost - 1, data = df_long)

# Choice vector
y <- df_long$choice

# Scale revenue and fuel_cost
rev_scale_factor <- 1000
cost_scale_factor <- 100
X_mat[,1] <- X_mat[,1]/rev_scale_factor
X_mat[,2] <- X_mat[,2]/cost_scale_factor

n_hauls <- length(unique(df_long$haul_id))
n_zones <- length(unique(df_long$zone_id))
```

## 4. Fitting the conditional logit model

```{r}
#| label: conditional-logit

# Specify negative log-likelihood function (conditional logit model)
nll_func <- function(par) {
  # Get parameters
  beta <- par$beta
  
  # Calculate utility for all choices
  # These are our data matrices passed in via 'data_list'
  V <- as.vector(X_mat %*% beta)
  
  # Reshape V
  V_mat <- matrix(V, nrow = n_hauls, ncol = n_zones, byrow = TRUE)  
  
  # Calculate the log-sum-exp (denominator)
  log_sum_exp <- log(RTMB::rowSums(exp(V_mat)))
  
  # Calculate Numerator (Utility of Chosen)
  V_chosen <- V[y == 1]
  
  nll <- -sum(V_chosen - log_sum_exp)
  
  return(nll)
}

# List of data to be used by the model function
# These are the wide matrices
data_list <- list(
  X_mat = X_mat,
  y = y
)

# List of parameters with their starting values
params <- list(
  beta = rep(0.01, ncol(X_mat))
)

# Compile TMB object
# Translates R model to C++ representation, automatically differentiates
obj <- RTMB::MakeADFun(
  func = nll_func,
  data = data_list,
  parameters = params
)

# Optimize
fit <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr
)

# Get parameter estimates and standard errors
sdr <- sdreport(obj)
print(sdr)
```

### Hausman-McFadden test

```{r}
#| label: hausman-mcfadden-test

# Helper function to refit model on subsets
fit_restricted_model <- function(drop_zone_id, full_data, original_params,
                                 rev_scale_factor, cost_scale_factor) {
  # Filter data - remove alternative from the choice set
  hauls_to_drop <- full_data %>%
    filter(zone_id == drop_zone_id & choice == 1) %>%
    pull(haul_id)
  
  # Create restricted dataset
  df_sub <- full_data %>%
    filter(zone_id != drop_zone_id) %>% # remove zone from options
    filter(!haul_id %in% hauls_to_drop) # remove hauls where zone was chosen
  
  # Recalculate dimensions
  n_hauls_sub <- length(unique(df_sub$haul_id))
  n_zones_sub <- length(unique(df_sub$zone_id))
  
  # Create new design matrix
  X_sub <- model.matrix(~ revenue + fuel_cost - 1, data = df_sub)
  X_sub[,1] <- X_sub[,1]/rev_scale_factor
  X_sub[,2] <- X_sub[,2]/cost_scale_factor
  
  nll_func_sub <- function(par) {
    beta <- par$beta
    V <- as.vector(X_sub %*% beta)
    # Reshape with new n_zones_sub
    V_mat <- matrix(V, nrow = n_hauls_sub, ncol = n_zones_sub, byrow = TRUE)  
    log_sum_exp <- log(RTMB::rowSums(exp(V_mat)))
    V_chosen <- V[df_sub$choice == 1]
    nll <- -sum(V_chosen - log_sum_exp)
    return(nll)
  }
  
  # Compile and optimize
  obj_sub <- RTMB::MakeADFun(
    func = nll_func_sub,
    parameters = original_params,
    silent = TRUE
  )
  
  fit_sub <- nlminb(start = obj_sub$par, 
                    objective = obj_sub$fn, 
                    gradient = obj_sub$gr)
  sdr_sub <- sdreport(obj_sub)
  
  return(list(pars = sdr_sub$par.fixed, cov = sdr_sub$cov.fixed))
}

# Parameters from the full model 
b_full <- sdr$par.fixed
V_full <- sdr$cov.fixed

# Fit restricted model
res_sub <- fit_restricted_model(drop_zone_id = 35, 
                                full_data = df_long, 
                                original_params = params,
                                rev_scale_factor = rev_scale_factor,
                                cost_scale_factor = cost_scale_factor)

b_sub <- res_sub$pars
V_sub <- res_sub$cov

# Difference in coefficients
diff_b <- b_sub - b_full
# Difference in covariance matrices
diff_V <- V_sub - V_full
# Hausman statistic
H_stat <- as.numeric(t(diff_b) %*% ginv(diff_V) %*% diff_b)
# Degrees of Freedom (number of parameters)
df_test <- length(b_full)
# P-value
p_val <- pchisq(H_stat, df = df_test, lower.tail = FALSE)

if (p_val < 0.05) {
  print("Reject null H. IIA assumption is violated.") 
} else {
  print("Fail to reject null H. IIA assumption is NOT violated.") 
}

```

### Moran's I on residuals

```{r}
#| label: morans-i

# Direct test for spatial autocorrelation
# Get predicted probabilities for the full model
beta_hat <- sdr$par.fixed
V_linear <- as.vector(X_mat %*% beta_hat)
V_mat <- matrix(V_linear, nrow = n_hauls, ncol = n_zones, byrow = TRUE)

# probability matrix
exp_V <- exp(V_mat)
sum_exp_V <- rowSums(exp_V)
P_mat <- exp_V / sum_exp_V

# Calculate residuals (observed choice - predicted probability)
y_mat <- matrix(y, nrow = n_hauls, ncol = n_zones, byrow = TRUE)
resid_mat <- y_mat - P_mat

# Aggregate residuals by zone
mean_resid <- colMeans(resid_mat)

# Add to spatial object
resid_sf <- grid_sf %>%
  st_as_sf() %>%
  mutate(residual = mean_resid)

# Run Moran's I
nb <- poly2nb(resid_sf) # neighbors list
lw <- nb2listw(nb, style = "W", zero.policy = TRUE)

# Global Moran's I test
moran_result <- moran.test(resid_sf$residual, lw, alternative = "two.sided")
print(moran_result)

# Visual check
ggplot(resid_sf) +
  geom_sf(aes(fill = residual)) +
  scale_fill_viridis_c() +
  ggtitle("Spatial Residuals of Conditional Logit") +
  theme_minimal()
```

## 5. Fit model that account for spatial autocorrelated errors

```{r}
#| label: spatial-logit

st_adjacent <- function(m, ...) st_relate(m, m, pattern="F***1****", ...)
A_ss = st_adjacent(grid_sf, sparse=TRUE)
A_ss = as(A_ss, "sparseMatrix") # 1 if cell i and cell j are neighbors

# Get design matrix
X_mat <- model.matrix(~ revenue + fuel_cost - 1, data = df_long)
X_mat[,1] <- X_mat[,1]/rev_scale_factor
X_mat[,2] <- X_mat[,2]/cost_scale_factor

# Make weight matrix for SAR process
W_ss = Diagonal(n=length(grid_sf), x=1/rowSums(A_ss)) %*% A_ss

# Define parameters
parlist = list(
  beta = rep(0, ncol(X_mat)), # Fixed effects
  logkappa = log(0.1), # Strength of spatial correlation
  logtau = log(1), # Overall variance
  # Random effects omega_s[i] represents the unobserved utility of grid cell i
  omega_s = 0.01 * rnorm(length(grid_sf)) 
)

y_mat <- matrix(y, nrow=n_hauls, ncol=n_zones, byrow = TRUE)
loc_t <- apply(y_mat, MARGIN=1, FUN = which.max)

spatial_nll_func <- function(parlist) {
  
  kappa <- exp(parlist$logkappa)
  tau2 <- exp(2 * parlist$logtau)
  
  # Make spatial precision for a SAR model
  I_ss <- Diagonal(n=length(grid_sf), x=1)
  Q_ss <- tau2 * t(I_ss - kappa*W_ss) %*% (I_ss - kappa*W_ss)
  # Get loglikeliood for omega as GMRF
  loglik1 <- dgmrf(parlist$omega_s, Q = Q_ss, log = TRUE)
  
  # Selection probability is constant in time for this example
  linear_pred <- (X_mat %*% parlist$beta)[,1] + parlist$omega_s
  V_mat <- matrix(linear_pred, nrow = n_hauls, ncol = n_zones, byrow = TRUE)
  phat_s <- log(RTMB::rowSums(exp(V_mat)))
  
  # Calculate probability for each choice
  chosen_indices <- cbind(seq_len(n_hauls), loc_t)
  V_chosen <- V_mat[chosen_indices]
  
  loglik2_i <- sum(V_chosen - phat_s)
  
  # Joint negative loglikelihood
  jnll = -1*loglik1 - loglik2_i
  return(jnll)
}

# Compile TMB ... uses Laplace approximation to marginalize across omega_s
obj = MakeADFun(
  func = spatial_nll_func,
  parameters = parlist,
  random = "omega_s",
  silent = TRUE
)

# Optimize
opt = nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr,
  control = list( trace = 1, eval.max = 1e4, iter.max = 1e4 )
)

# get SEs
# beta_revenue <- 0.001
# beta_fuel_cost <- -0.05
sdrep = sdreport(obj)

# get estimates
parhat = obj$env$parList()

plotgrid = st_sf(
  grid_sf,
  revenue = base_revenue,
  hidden_util = spatial_error_term,
  omegahat_s = parhat$omega_s,
  effort = colSums(y_mat)
)
plot(plotgrid)

```

```{r}
#| label: spatial-logit-morans

# Save estimated parameters
par_spatial <- obj$env$parList()
beta_spatial <- par_spatial$beta
omega_spatial <- par_spatial$omega_s

# Calculate linear predictor
V_linear_spatial <- (X_mat %*% beta_spatial)[,1] + omega_spatial
V_mat_spatial <- matrix(V_linear_spatial, nrow = n_hauls, ncol = n_zones, byrow = TRUE)

# Calculate probabilities - softmax function
exp_V_spatial <- exp(V_mat_spatial)
sum_exp_spatial <- rowSums(exp_V_spatial)
P_mat_spatial <- exp_V_spatial / sum_exp_spatial

# Calculate average residuals by zone
y_mat <- matrix(y, nrow = n_hauls, ncol = n_zones, byrow = TRUE)
resid_spatial_mat <- y_mat - P_mat_spatial
mean_resid_spatial <- colMeans(resid_spatial_mat)

# Run Moran's I
resid_sf_final <- grid_sf %>%
  st_as_sf() %>%
  mutate(residual = mean_resid_spatial)

# Neighbors
nb_check <- poly2nb(resid_sf_final)
lw_check <- nb2listw(nb_check, style = "W", zero.policy = TRUE)
moran_check <- moran.test(resid_sf_final$residual, lw_check)
print(moran_check)

ggplot(resid_sf_final) + 
  geom_sf(aes(fill = residual)) +
  scale_fill_viridis_c() +
  ggtitle("After: Spatial Model Errors") + theme_void()
```

```{r}
#| label: welfare-analysis

# Estimated scaled params
coeff_scaled <- sdr$par.fixed

# Calculate unscaled values
beta_rev_unscaled <- coeff_scaled["beta_revenue"] / rev_scale_factor

# Calculate before and after utility (not the covartiate matrices are scaled)
utility_before_scaled <- (
(coeff_scaled["beta_revenue"] * revenue_mat) +
(coeff_scaled["beta_fuel_cost"] * fuel_cost_mat) +
(coeff_scaled["beta_port_B"] * port_B_mat) +
(coeff_scaled["beta_port_C"] * port_C_mat))

# Log-sum before (by row)
logsum_before <- apply(utility_before_scaled, 1, function(x) log(sum(exp(x))))

# Zones to close
zones_to_close <- c(1, 2)

# Create copy of utility matrix
utility_after_scaled <- utility_before_scaled

# Set utility of closed zones to -Inf because exp(-Inf) = 0
utility_after_scaled[, zones_to_close] <- -Inf

# Log-sum after (by row)
logsum_after <- apply(utility_after_scaled,
1,
function(x) log(sum(exp(x))))

# Change in utility
delta_utility <- logsum_after - logsum_before

# Convert to dollars (utility / (utility/$))
welfare_change_per_haul <- delta_utility/beta_rev_unscaled

# Summary
total_loss <- sum(welfare_change_per_haul)
avg_loss <- mean(welfare_change_per_haul)
n_hauls_affected <- sum(welfare_change_per_haul < 0)

cat("Total loss over all hauls: $", -(total_loss), "\n")
cat("Average loss per haul: $", -(avg_loss), "\n")
```

The average loss is \$10,794 dollars, which means that fishers would have to be compensated that amount to return them to their original welfare level prior to closing fishing zones.
